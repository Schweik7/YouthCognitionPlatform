#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‘æ–‡æ™ºåŠ›æµ‹éªŒå›¾ç‰‡çˆ¬å–è„šæœ¬
ä½¿ç”¨Playwrightè‡ªåŠ¨åŒ–è·å–é¢˜ç›®å’Œé€‰é¡¹å›¾ç‰‡
"""

import asyncio
import aiohttp
import os
import re
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse
from playwright.async_api import async_playwright

class RavenImageScraper:
    def __init__(self, base_dir="backend/data/raven_test"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨æ¯é¢˜çš„å›¾ç‰‡ä¿¡æ¯
        self.current_question = 1
        self.question_images = []  # å½“å‰é¢˜ç›®æ”¶é›†åˆ°çš„å›¾ç‰‡URL
        
    async def download_image(self, session, url, filepath):
        """ä¸‹è½½å•ä¸ªå›¾ç‰‡"""
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    with open(filepath, 'wb') as f:
                        f.write(content)
                    print(f"âœ“ ä¸‹è½½æˆåŠŸ: {filepath}")
                    return True
                else:
                    print(f"âœ— ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status})")
                    return False
        except Exception as e:
            print(f"âœ— ä¸‹è½½å¼‚å¸¸: {url} - {str(e)}")
            return False
    
    def normalize_filename(self, url, question_num, is_main=False, option_num=None):
        """è§„èŒƒåŒ–æ–‡ä»¶å"""
        # è§£æURLè·å–æ–‡ä»¶æ‰©å±•å
        parsed_url = urlparse(url)
        filename = os.path.basename(parsed_url.path)
        
        # æå–æ‰©å±•å
        ext = os.path.splitext(filename)[1]
        if not ext:
            ext = '.jpg'  # é»˜è®¤æ‰©å±•å
        
        # ç”Ÿæˆè§„èŒƒåŒ–çš„æ–‡ä»¶å
        if is_main:
            return f"question_{question_num:02d}_main{ext}"
        else:
            return f"question_{question_num:02d}_option_{option_num}{ext}"
    
    async def handle_network_request(self, request):
        """å¤„ç†ç½‘ç»œè¯·æ±‚ï¼Œæ•è·å›¾ç‰‡URL"""
        url = request.url
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡è¯·æ±‚
        if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
            print(f"ğŸ–¼ï¸  å‘ç°å›¾ç‰‡URL: {url}")
            
            # æ·»åŠ åˆ°å½“å‰é¢˜ç›®çš„å›¾ç‰‡åˆ—è¡¨
            self.question_images.append(url)
    
    async def get_image_src_from_page(self, page):
        """ä»é¡µé¢DOMä¸­ç›´æ¥è·å–å›¾ç‰‡src"""
        try:
            # è·å–æ‰€æœ‰imgå…ƒç´ çš„srcå±æ€§
            img_srcs = await page.evaluate("""
                () => {
                    const imgs = document.querySelectorAll('img');
                    return Array.from(imgs).map(img => img.src).filter(src => src && src.length > 0);
                }
            """)
            return img_srcs
        except Exception as e:
            print(f"  è·å–å›¾ç‰‡srcå¤±è´¥: {e}")
            return []

    async def process_question(self, page, question_num):
        """å¤„ç†å•ä¸ªé¢˜ç›®"""
        print(f"\nğŸ“ å¤„ç†ç¬¬ {question_num} é¢˜...")
        self.current_question = question_num
        self.question_images.clear()
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        await page.wait_for_load_state('networkidle')
        await asyncio.sleep(3)  # ç­‰å¾…å›¾ç‰‡åŠ è½½
        
        # ä»é¡µé¢DOMä¸­è·å–å›¾ç‰‡URL
        page_images = await self.get_image_src_from_page(page)
        
        # è¿‡æ»¤å‡ºé¢˜ç›®ç›¸å…³çš„å›¾ç‰‡ï¼ˆé€šå¸¸åŒ…å«ç‰¹å®šè·¯å¾„ï¼‰
        question_images = [img for img in page_images if any(keyword in img.lower() for keyword in ['iq', 'test', 'question', 'raven']) or img.startswith('data:')]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå›¾ç‰‡ï¼Œä½¿ç”¨æ‰€æœ‰å›¾ç‰‡
        if not question_images:
            question_images = [img for img in page_images if not any(skip in img.lower() for skip in ['icon', 'logo', 'avatar', 'banner'])]
        
        print(f"  å‘ç° {len(question_images)} å¼ é¢˜ç›®ç›¸å…³å›¾ç‰‡")
        
        if len(question_images) >= 7:  # é¢„æœŸæœ‰7å¼ å›¾ç‰‡ï¼ˆ1é¢˜ç›®+6é€‰é¡¹ï¼‰
            # ä¸‹è½½å›¾ç‰‡
            async with aiohttp.ClientSession() as session:
                # ä¸‹è½½ä¸»é¢˜å›¾ç‰‡ï¼ˆç¬¬ä¸€å¼ é€šå¸¸æ˜¯é¢˜ç›®ï¼‰
                main_image_url = question_images[0]
                main_filename = self.normalize_filename(main_image_url, question_num, is_main=True)
                main_path = self.base_dir / main_filename
                await self.download_image(session, main_image_url, main_path)
                
                # ä¸‹è½½é€‰é¡¹å›¾ç‰‡ï¼ˆæ¥ä¸‹æ¥çš„6å¼ ï¼‰
                for i, option_url in enumerate(question_images[1:7], 1):
                    option_filename = self.normalize_filename(option_url, question_num, option_num=i)
                    option_path = self.base_dir / option_filename
                    await self.download_image(session, option_url, option_path)
        
        else:
            print(f"  âš ï¸  å›¾ç‰‡æ•°é‡ä¸è¶³ï¼ŒæœŸæœ›7å¼ ï¼Œå®é™…{len(question_images)}å¼ ")
            # ä»ç„¶å°è¯•ä¸‹è½½å¯ç”¨çš„å›¾ç‰‡
            async with aiohttp.ClientSession() as session:
                for i, img_url in enumerate(question_images):
                    filename = f"question_{question_num:02d}_img_{i+1:02d}.jpg"
                    filepath = self.base_dir / filename
                    await self.download_image(session, img_url, filepath)
    
    async def click_next_question(self, page):
        """ç‚¹å‡»é€‰é¡¹è¿›å…¥ä¸‹ä¸€é¢˜"""
        try:
            # åŸºäºBrowser MCPçš„è§‚å¯Ÿï¼Œé€‰é¡¹å›¾ç‰‡åœ¨é¡µé¢ä¸­çš„ç‰¹å®šä½ç½®
            # å°è¯•æ‰¾åˆ°é€‰é¡¹å›¾ç‰‡å¹¶ç‚¹å‡»ç¬¬ä¸€ä¸ª
            
            # æ–¹æ³•1ï¼šé€šè¿‡JavaScriptè·å–æ‰€æœ‰å›¾ç‰‡å¹¶ç‚¹å‡»ç¬¬äºŒä¸ªï¼ˆç¬¬ä¸€ä¸ªæ˜¯é¢˜ç›®å›¾ç‰‡ï¼‰
            clicked = await page.evaluate("""
                () => {
                    const imgs = document.querySelectorAll('img');
                    const validImgs = Array.from(imgs).filter(img => 
                        img.src && 
                        !img.src.includes('icon') && 
                        !img.src.includes('logo') &&
                        img.offsetWidth > 30 && 
                        img.offsetHeight > 30
                    );
                    
                    if (validImgs.length >= 7) {
                        // ç‚¹å‡»ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆç¬¬ä¸€å¼ æ˜¯é¢˜ç›®å›¾ç‰‡ï¼Œç¬¬äºŒå¼ æ˜¯ç¬¬ä¸€ä¸ªé€‰é¡¹ï¼‰
                        validImgs[1].click();
                        return true;
                    }
                    return false;
                }
            """)
            
            if clicked:
                print("  âœ“ ç‚¹å‡»é€‰é¡¹æˆåŠŸ")
            else:
                # æ–¹æ³•2ï¼šå°è¯•ç‚¹å‡»ä»»æ„å¯ç‚¹å‡»çš„å›¾ç‰‡
                images = await page.query_selector_all('img')
                if len(images) >= 2:
                    # ç‚¹å‡»ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆå‡è®¾ç¬¬ä¸€å¼ æ˜¯é¢˜ç›®å›¾ç‰‡ï¼‰
                    await images[1].click()
                    print("  âœ“ ç‚¹å‡»å›¾ç‰‡é€‰é¡¹")
                else:
                    print("  âš ï¸  æœªæ‰¾åˆ°è¶³å¤Ÿçš„å›¾ç‰‡é€‰é¡¹")
            
            # ç­‰å¾…é¡µé¢è·³è½¬
            await page.wait_for_load_state('networkidle')
            await asyncio.sleep(2)
            
        except Exception as e:
            print(f"  âœ— ç‚¹å‡»é€‰é¡¹å¤±è´¥: {str(e)}")
    
    async def scrape_all_questions(self, url="https://minke8.cn/iq1.html", max_questions=72):
        """çˆ¬å–æ‰€æœ‰é¢˜ç›®"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ç‘æ–‡æ™ºåŠ›æµ‹éªŒå›¾ç‰‡...")
        print(f"ç›®æ ‡URL: {url}")
        print(f"ä¿å­˜è·¯å¾„: {self.base_dir}")
        print(f"é¢„è®¡é¢˜ç›®æ•°: {max_questions}")
        
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = await p.chromium.launch(
                headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
                args=[
                    '--disable-dev-shm-usage',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            
            try:
                context = await browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                )
                
                page = await context.new_page()
                
                # è®¾ç½®ç½‘ç»œç›‘å¬
                page.on('request', self.handle_network_request)
                
                # è®¿é—®é¦–é¡µ
                print(f"\nğŸŒ è®¿é—®: {url}")
                await page.goto(url, wait_until='networkidle')
                
                # ç‚¹å‡»"å¼€å§‹æµ‹è¯•"æŒ‰é’®
                try:
                    start_button = await page.query_selector('text=å¼€å§‹æµ‹è¯•')
                    if start_button:
                        await start_button.click()
                        print("âœ“ ç‚¹å‡»å¼€å§‹æµ‹è¯•æŒ‰é’®")
                        await page.wait_for_load_state('networkidle')
                        await asyncio.sleep(2)
                    else:
                        print("âš ï¸  æœªæ‰¾åˆ°å¼€å§‹æµ‹è¯•æŒ‰é’®")
                except Exception as e:
                    print(f"ç‚¹å‡»å¼€å§‹æµ‹è¯•æŒ‰é’®å¤±è´¥: {e}")
                
                # å¤„ç†æ¯ä¸ªé¢˜ç›®
                for question_num in range(1, max_questions + 1):
                    try:
                        await self.process_question(page, question_num)
                        
                        if question_num < max_questions:
                            await self.click_next_question(page)
                            
                    except Exception as e:
                        print(f"  âœ— å¤„ç†ç¬¬{question_num}é¢˜æ—¶å‡ºé”™: {str(e)}")
                        # å°è¯•ç»§ç»­ä¸‹ä¸€é¢˜
                        try:
                            await self.click_next_question(page)
                        except:
                            pass
                        continue
                
                print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¤„ç† {max_questions} é¢˜")
                
            finally:
                await browser.close()


async def main():
    """ä¸»å‡½æ•°"""
    scraper = RavenImageScraper()
    await scraper.scrape_all_questions()


if __name__ == "__main__":
    asyncio.run(main())